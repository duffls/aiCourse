{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a140f90c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_1 (SimpleRNN)     (None, 4)                 60        \n",
      "=================================================================\n",
      "Total params: 60\n",
      "Trainable params: 60\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# RNN 층에 대핚 코드\n",
    "from keras.models import Sequential\n",
    "from keras.layers import SimpleRNN\n",
    "\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(4, input_shape=(2,10)))\n",
    "# model.add(SimpleRNN(3, input_length=2, input_dim=10))와 동일함.\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a9fdb8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_2 (SimpleRNN)     (8, 3)                    42        \n",
      "=================================================================\n",
      "Total params: 42\n",
      "Trainable params: 42\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import SimpleRNN\n",
    "\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(3, batch_input_shape=(8, 2,10))) # return_sequences=False 인 경우 (batch_size)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fc51c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_3 (SimpleRNN)     (8, 2, 3)                 42        \n",
      "=================================================================\n",
      "Total params: 42\n",
      "Trainable params: 42\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import SimpleRNN\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(3, batch_input_shape=(8,2,10), return_sequences=True))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b481e6b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e15f3389",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "text=\"\"\"경마장에 있는 말이 뛰고 있다\\n 그의 말이 법이다\\n 가는 말이 고와야 오는 말이 곱다\\n\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08758e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기 : 12\n",
      "{'말이': 1, '경마장에': 2, '있는': 3, '뛰고': 4, '있다': 5, '그의': 6, '법이다': 7, '가는': 8, '고와야': 9, '오는': 10, '곱다': 11}\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer() \n",
    "t = Tokenizer()\n",
    "# 주어진 데이터에 맞게 단어와 단어 인덱스를 구축하여 리스트 형식으로 데이터를 전달한다.\n",
    "t.fit_on_texts([text])\n",
    "vocab_size = len(t.word_index) + 1\n",
    "print(f'단어 집합의 크기 : {vocab_size:d}') # % vocab_size)\n",
    "print(t.word_index) # 각 단어와 단어에 부여된 정수 인덱스 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8cb2ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6b1e8c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 1, 4, 5]\n",
      "[6, 1, 7]\n",
      "[8, 1, 9, 10, 1, 11]\n",
      "[]\n",
      "학습에 사용핛 샘플의 개수: 11\n"
     ]
    }
   ],
   "source": [
    "sequences = list()\n",
    "for line in text.split('\\n'): # Wn을 기준으로 문장 토큰화\n",
    "    #print(t.texts_to_sequences([line]))\n",
    "    encoded = t.texts_to_sequences([line])[0]\n",
    "    print(encoded)\n",
    "    for i in range(1, len(encoded)):\n",
    "        sequence = encoded[:i+1]\n",
    "        sequences.append(sequence)\n",
    "print(f'학습에 사용할 샘플의 개수: {len(sequences):d}') # % len(sequences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71989260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 3], [2, 3, 1], [2, 3, 1, 4], [2, 3, 1, 4, 5], [6, 1], [6, 1, 7], [8, 1], [8, 1, 9], [8, 1, 9, 10], [8, 1, 9, 10, 1], [8, 1, 9, 10, 1, 11]]\n"
     ]
    }
   ],
   "source": [
    "print(sequences) # 전체 샘플을 출력\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85375c12",
   "metadata": {},
   "source": [
    "- 위의 데이터는 아직 레이블로 사용될 단어를 분리하지 않은 훈렦 데이터이다. [2, 3]은 [경마장에, 있는]에\n",
    "해당되며 [2, 3, 1]은 [경마장에, 있는, 말이]에 해당된다. 젂체 훈렦 데이터에 대해서 맦 우측에 있는\n",
    "단어에 대해서맊 레이블로 분리해야 핚다.\n",
    "- 우선 젂체 샘플에 대해서 길이를 일치시켜 준다. 가장 긴 샘플의 길이를 기준으로 핚다. 현재 육안으로\n",
    "봤을 때, 길이가 가장 긴 샘플은 [8, 1, 9, 10, 1, 11]이고 길이는 6이다.\n",
    "max_len=max(len(l) for l in sequences) # 모든 샘플에서 길이가 가장 긴 샘플의 길이 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd985840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "샘플의 최대 길이 : 6\n"
     ]
    }
   ],
   "source": [
    "max_len=max(len(l) for l in sequences) # 모든 샘플에서 길이가 가장 긴 샘플의 길이 출력\n",
    "print(f'샘플의 최대 길이 : {max_len}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0c9044",
   "metadata": {},
   "source": [
    "# pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e6fc2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0  2  3]\n",
      " [ 0  0  0  2  3  1]\n",
      " [ 0  0  2  3  1  4]\n",
      " [ 0  2  3  1  4  5]\n",
      " [ 0  0  0  0  6  1]\n",
      " [ 0  0  0  6  1  7]\n",
      " [ 0  0  0  0  8  1]\n",
      " [ 0  0  0  8  1  9]\n",
      " [ 0  0  8  1  9 10]\n",
      " [ 0  8  1  9 10  1]\n",
      " [ 8  1  9 10  1 11]]\n"
     ]
    }
   ],
   "source": [
    "# 젂체 샘플의 길이를 6으로 패딩\n",
    "sequences = pad_sequences(sequences, maxlen=max_len, padding='pre')\n",
    "print(sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f33c198f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0  2]\n",
      " [ 0  0  0  2  3]\n",
      " [ 0  0  2  3  1]\n",
      " [ 0  2  3  1  4]\n",
      " [ 0  0  0  0  6]\n",
      " [ 0  0  0  6  1]\n",
      " [ 0  0  0  0  8]\n",
      " [ 0  0  0  8  1]\n",
      " [ 0  0  8  1  9]\n",
      " [ 0  8  1  9 10]\n",
      " [ 8  1  9 10  1]]\n",
      "[ 3  1  4  5  1  7  1  9 10  1 11]\n"
     ]
    }
   ],
   "source": [
    "sequences = np.array(sequences)\n",
    "X = sequences[:,:-1] # 학습 데이터\n",
    "y = sequences[:,-1] # 정답(Label) 데이터\n",
    "print(X)\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a8745f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "y = to_categorical(y, num_classes=vocab_size) # 원-핫 인코딩 수행\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7d42a4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 5, 6)              72        \n",
      "_________________________________________________________________\n",
      "simple_rnn_3 (SimpleRNN)     (None, 32)                1248      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 12)                396       \n",
      "=================================================================\n",
      "Total params: 1,716\n",
      "Trainable params: 1,716\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, SimpleRNN\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 6, input_length=max_len-1))\n",
    "model.add(SimpleRNN(32))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "334b02e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1/1 - 0s - loss: 2.4856 - accuracy: 0.0909\n",
      "Epoch 2/200\n",
      "1/1 - 0s - loss: 2.4748 - accuracy: 0.0909\n",
      "Epoch 3/200\n",
      "1/1 - 0s - loss: 2.4639 - accuracy: 0.0909\n",
      "Epoch 4/200\n",
      "1/1 - 0s - loss: 2.4528 - accuracy: 0.1818\n",
      "Epoch 5/200\n",
      "1/1 - 0s - loss: 2.4415 - accuracy: 0.3636\n",
      "Epoch 6/200\n",
      "1/1 - 0s - loss: 2.4298 - accuracy: 0.3636\n",
      "Epoch 7/200\n",
      "1/1 - 0s - loss: 2.4178 - accuracy: 0.3636\n",
      "Epoch 8/200\n",
      "1/1 - 0s - loss: 2.4053 - accuracy: 0.3636\n",
      "Epoch 9/200\n",
      "1/1 - 0s - loss: 2.3924 - accuracy: 0.3636\n",
      "Epoch 10/200\n",
      "1/1 - 0s - loss: 2.3789 - accuracy: 0.3636\n",
      "Epoch 11/200\n",
      "1/1 - 0s - loss: 2.3648 - accuracy: 0.3636\n",
      "Epoch 12/200\n",
      "1/1 - 0s - loss: 2.3500 - accuracy: 0.3636\n",
      "Epoch 13/200\n",
      "1/1 - 0s - loss: 2.3346 - accuracy: 0.3636\n",
      "Epoch 14/200\n",
      "1/1 - 0s - loss: 2.3183 - accuracy: 0.3636\n",
      "Epoch 15/200\n",
      "1/1 - 0s - loss: 2.3013 - accuracy: 0.3636\n",
      "Epoch 16/200\n",
      "1/1 - 0s - loss: 2.2835 - accuracy: 0.3636\n",
      "Epoch 17/200\n",
      "1/1 - 0s - loss: 2.2648 - accuracy: 0.3636\n",
      "Epoch 18/200\n",
      "1/1 - 0s - loss: 2.2453 - accuracy: 0.3636\n",
      "Epoch 19/200\n",
      "1/1 - 0s - loss: 2.2251 - accuracy: 0.3636\n",
      "Epoch 20/200\n",
      "1/1 - 0s - loss: 2.2041 - accuracy: 0.3636\n",
      "Epoch 21/200\n",
      "1/1 - 0s - loss: 2.1826 - accuracy: 0.3636\n",
      "Epoch 22/200\n",
      "1/1 - 0s - loss: 2.1605 - accuracy: 0.3636\n",
      "Epoch 23/200\n",
      "1/1 - 0s - loss: 2.1382 - accuracy: 0.3636\n",
      "Epoch 24/200\n",
      "1/1 - 0s - loss: 2.1158 - accuracy: 0.3636\n",
      "Epoch 25/200\n",
      "1/1 - 0s - loss: 2.0935 - accuracy: 0.3636\n",
      "Epoch 26/200\n",
      "1/1 - 0s - loss: 2.0718 - accuracy: 0.3636\n",
      "Epoch 27/200\n",
      "1/1 - 0s - loss: 2.0507 - accuracy: 0.3636\n",
      "Epoch 28/200\n",
      "1/1 - 0s - loss: 2.0308 - accuracy: 0.3636\n",
      "Epoch 29/200\n",
      "1/1 - 0s - loss: 2.0121 - accuracy: 0.3636\n",
      "Epoch 30/200\n",
      "1/1 - 0s - loss: 1.9948 - accuracy: 0.3636\n",
      "Epoch 31/200\n",
      "1/1 - 0s - loss: 1.9790 - accuracy: 0.3636\n",
      "Epoch 32/200\n",
      "1/1 - 0s - loss: 1.9647 - accuracy: 0.3636\n",
      "Epoch 33/200\n",
      "1/1 - 0s - loss: 1.9515 - accuracy: 0.3636\n",
      "Epoch 34/200\n",
      "1/1 - 0s - loss: 1.9393 - accuracy: 0.3636\n",
      "Epoch 35/200\n",
      "1/1 - 0s - loss: 1.9278 - accuracy: 0.3636\n",
      "Epoch 36/200\n",
      "1/1 - 0s - loss: 1.9165 - accuracy: 0.3636\n",
      "Epoch 37/200\n",
      "1/1 - 0s - loss: 1.9052 - accuracy: 0.3636\n",
      "Epoch 38/200\n",
      "1/1 - 0s - loss: 1.8938 - accuracy: 0.3636\n",
      "Epoch 39/200\n",
      "1/1 - 0s - loss: 1.8822 - accuracy: 0.3636\n",
      "Epoch 40/200\n",
      "1/1 - 0s - loss: 1.8703 - accuracy: 0.3636\n",
      "Epoch 41/200\n",
      "1/1 - 0s - loss: 1.8582 - accuracy: 0.3636\n",
      "Epoch 42/200\n",
      "1/1 - 0s - loss: 1.8461 - accuracy: 0.3636\n",
      "Epoch 43/200\n",
      "1/1 - 0s - loss: 1.8340 - accuracy: 0.3636\n",
      "Epoch 44/200\n",
      "1/1 - 0s - loss: 1.8220 - accuracy: 0.3636\n",
      "Epoch 45/200\n",
      "1/1 - 0s - loss: 1.8102 - accuracy: 0.3636\n",
      "Epoch 46/200\n",
      "1/1 - 0s - loss: 1.7985 - accuracy: 0.3636\n",
      "Epoch 47/200\n",
      "1/1 - 0s - loss: 1.7871 - accuracy: 0.3636\n",
      "Epoch 48/200\n",
      "1/1 - 0s - loss: 1.7757 - accuracy: 0.3636\n",
      "Epoch 49/200\n",
      "1/1 - 0s - loss: 1.7642 - accuracy: 0.3636\n",
      "Epoch 50/200\n",
      "1/1 - 0s - loss: 1.7527 - accuracy: 0.3636\n",
      "Epoch 51/200\n",
      "1/1 - 0s - loss: 1.7409 - accuracy: 0.3636\n",
      "Epoch 52/200\n",
      "1/1 - 0s - loss: 1.7288 - accuracy: 0.3636\n",
      "Epoch 53/200\n",
      "1/1 - 0s - loss: 1.7162 - accuracy: 0.3636\n",
      "Epoch 54/200\n",
      "1/1 - 0s - loss: 1.7033 - accuracy: 0.3636\n",
      "Epoch 55/200\n",
      "1/1 - 0s - loss: 1.6899 - accuracy: 0.4545\n",
      "Epoch 56/200\n",
      "1/1 - 0s - loss: 1.6760 - accuracy: 0.4545\n",
      "Epoch 57/200\n",
      "1/1 - 0s - loss: 1.6618 - accuracy: 0.4545\n",
      "Epoch 58/200\n",
      "1/1 - 0s - loss: 1.6471 - accuracy: 0.4545\n",
      "Epoch 59/200\n",
      "1/1 - 0s - loss: 1.6320 - accuracy: 0.4545\n",
      "Epoch 60/200\n",
      "1/1 - 0s - loss: 1.6165 - accuracy: 0.4545\n",
      "Epoch 61/200\n",
      "1/1 - 0s - loss: 1.6006 - accuracy: 0.4545\n",
      "Epoch 62/200\n",
      "1/1 - 0s - loss: 1.5843 - accuracy: 0.4545\n",
      "Epoch 63/200\n",
      "1/1 - 0s - loss: 1.5676 - accuracy: 0.4545\n",
      "Epoch 64/200\n",
      "1/1 - 0s - loss: 1.5504 - accuracy: 0.4545\n",
      "Epoch 65/200\n",
      "1/1 - 0s - loss: 1.5329 - accuracy: 0.4545\n",
      "Epoch 66/200\n",
      "1/1 - 0s - loss: 1.5149 - accuracy: 0.4545\n",
      "Epoch 67/200\n",
      "1/1 - 0s - loss: 1.4966 - accuracy: 0.4545\n",
      "Epoch 68/200\n",
      "1/1 - 0s - loss: 1.4778 - accuracy: 0.4545\n",
      "Epoch 69/200\n",
      "1/1 - 0s - loss: 1.4588 - accuracy: 0.4545\n",
      "Epoch 70/200\n",
      "1/1 - 0s - loss: 1.4394 - accuracy: 0.4545\n",
      "Epoch 71/200\n",
      "1/1 - 0s - loss: 1.4197 - accuracy: 0.4545\n",
      "Epoch 72/200\n",
      "1/1 - 0s - loss: 1.3998 - accuracy: 0.4545\n",
      "Epoch 73/200\n",
      "1/1 - 0s - loss: 1.3797 - accuracy: 0.4545\n",
      "Epoch 74/200\n",
      "1/1 - 0s - loss: 1.3595 - accuracy: 0.4545\n",
      "Epoch 75/200\n",
      "1/1 - 0s - loss: 1.3391 - accuracy: 0.4545\n",
      "Epoch 76/200\n",
      "1/1 - 0s - loss: 1.3187 - accuracy: 0.4545\n",
      "Epoch 77/200\n",
      "1/1 - 0s - loss: 1.2981 - accuracy: 0.5455\n",
      "Epoch 78/200\n",
      "1/1 - 0s - loss: 1.2776 - accuracy: 0.5455\n",
      "Epoch 79/200\n",
      "1/1 - 0s - loss: 1.2570 - accuracy: 0.5455\n",
      "Epoch 80/200\n",
      "1/1 - 0s - loss: 1.2364 - accuracy: 0.6364\n",
      "Epoch 81/200\n",
      "1/1 - 0s - loss: 1.2159 - accuracy: 0.6364\n",
      "Epoch 82/200\n",
      "1/1 - 0s - loss: 1.1954 - accuracy: 0.6364\n",
      "Epoch 83/200\n",
      "1/1 - 0s - loss: 1.1750 - accuracy: 0.6364\n",
      "Epoch 84/200\n",
      "1/1 - 0s - loss: 1.1547 - accuracy: 0.6364\n",
      "Epoch 85/200\n",
      "1/1 - 0s - loss: 1.1344 - accuracy: 0.6364\n",
      "Epoch 86/200\n",
      "1/1 - 0s - loss: 1.1143 - accuracy: 0.7273\n",
      "Epoch 87/200\n",
      "1/1 - 0s - loss: 1.0943 - accuracy: 0.7273\n",
      "Epoch 88/200\n",
      "1/1 - 0s - loss: 1.0745 - accuracy: 0.7273\n",
      "Epoch 89/200\n",
      "1/1 - 0s - loss: 1.0549 - accuracy: 0.7273\n",
      "Epoch 90/200\n",
      "1/1 - 0s - loss: 1.0355 - accuracy: 0.7273\n",
      "Epoch 91/200\n",
      "1/1 - 0s - loss: 1.0162 - accuracy: 0.7273\n",
      "Epoch 92/200\n",
      "1/1 - 0s - loss: 0.9972 - accuracy: 0.7273\n",
      "Epoch 93/200\n",
      "1/1 - 0s - loss: 0.9784 - accuracy: 0.7273\n",
      "Epoch 94/200\n",
      "1/1 - 0s - loss: 0.9599 - accuracy: 0.7273\n",
      "Epoch 95/200\n",
      "1/1 - 0s - loss: 0.9417 - accuracy: 0.7273\n",
      "Epoch 96/200\n",
      "1/1 - 0s - loss: 0.9237 - accuracy: 0.7273\n",
      "Epoch 97/200\n",
      "1/1 - 0s - loss: 0.9060 - accuracy: 0.7273\n",
      "Epoch 98/200\n",
      "1/1 - 0s - loss: 0.8886 - accuracy: 0.7273\n",
      "Epoch 99/200\n",
      "1/1 - 0s - loss: 0.8716 - accuracy: 0.8182\n",
      "Epoch 100/200\n",
      "1/1 - 0s - loss: 0.8548 - accuracy: 0.8182\n",
      "Epoch 101/200\n",
      "1/1 - 0s - loss: 0.8383 - accuracy: 0.8182\n",
      "Epoch 102/200\n",
      "1/1 - 0s - loss: 0.8222 - accuracy: 0.8182\n",
      "Epoch 103/200\n",
      "1/1 - 0s - loss: 0.8063 - accuracy: 0.8182\n",
      "Epoch 104/200\n",
      "1/1 - 0s - loss: 0.7908 - accuracy: 0.8182\n",
      "Epoch 105/200\n",
      "1/1 - 0s - loss: 0.7756 - accuracy: 0.8182\n",
      "Epoch 106/200\n",
      "1/1 - 0s - loss: 0.7607 - accuracy: 0.8182\n",
      "Epoch 107/200\n",
      "1/1 - 0s - loss: 0.7461 - accuracy: 0.8182\n",
      "Epoch 108/200\n",
      "1/1 - 0s - loss: 0.7318 - accuracy: 0.8182\n",
      "Epoch 109/200\n",
      "1/1 - 0s - loss: 0.7178 - accuracy: 0.8182\n",
      "Epoch 110/200\n",
      "1/1 - 0s - loss: 0.7042 - accuracy: 0.8182\n",
      "Epoch 111/200\n",
      "1/1 - 0s - loss: 0.6908 - accuracy: 0.8182\n",
      "Epoch 112/200\n",
      "1/1 - 0s - loss: 0.6777 - accuracy: 0.8182\n",
      "Epoch 113/200\n",
      "1/1 - 0s - loss: 0.6649 - accuracy: 0.8182\n",
      "Epoch 114/200\n",
      "1/1 - 0s - loss: 0.6524 - accuracy: 0.8182\n",
      "Epoch 115/200\n",
      "1/1 - 0s - loss: 0.6401 - accuracy: 0.9091\n",
      "Epoch 116/200\n",
      "1/1 - 0s - loss: 0.6282 - accuracy: 0.9091\n",
      "Epoch 117/200\n",
      "1/1 - 0s - loss: 0.6165 - accuracy: 0.9091\n",
      "Epoch 118/200\n",
      "1/1 - 0s - loss: 0.6050 - accuracy: 0.9091\n",
      "Epoch 119/200\n",
      "1/1 - 0s - loss: 0.5939 - accuracy: 0.9091\n",
      "Epoch 120/200\n",
      "1/1 - 0s - loss: 0.5830 - accuracy: 0.9091\n",
      "Epoch 121/200\n",
      "1/1 - 0s - loss: 0.5723 - accuracy: 0.9091\n",
      "Epoch 122/200\n",
      "1/1 - 0s - loss: 0.5619 - accuracy: 0.9091\n",
      "Epoch 123/200\n",
      "1/1 - 0s - loss: 0.5517 - accuracy: 0.9091\n",
      "Epoch 124/200\n",
      "1/1 - 0s - loss: 0.5418 - accuracy: 0.9091\n",
      "Epoch 125/200\n",
      "1/1 - 0s - loss: 0.5321 - accuracy: 0.9091\n",
      "Epoch 126/200\n",
      "1/1 - 0s - loss: 0.5226 - accuracy: 0.9091\n",
      "Epoch 127/200\n",
      "1/1 - 0s - loss: 0.5133 - accuracy: 0.9091\n",
      "Epoch 128/200\n",
      "1/1 - 0s - loss: 0.5043 - accuracy: 0.9091\n",
      "Epoch 129/200\n",
      "1/1 - 0s - loss: 0.4954 - accuracy: 0.9091\n",
      "Epoch 130/200\n",
      "1/1 - 0s - loss: 0.4867 - accuracy: 0.9091\n",
      "Epoch 131/200\n",
      "1/1 - 0s - loss: 0.4783 - accuracy: 0.9091\n",
      "Epoch 132/200\n",
      "1/1 - 0s - loss: 0.4700 - accuracy: 0.9091\n",
      "Epoch 133/200\n",
      "1/1 - 0s - loss: 0.4619 - accuracy: 0.9091\n",
      "Epoch 134/200\n",
      "1/1 - 0s - loss: 0.4539 - accuracy: 0.9091\n",
      "Epoch 135/200\n",
      "1/1 - 0s - loss: 0.4462 - accuracy: 0.9091\n",
      "Epoch 136/200\n",
      "1/1 - 0s - loss: 0.4386 - accuracy: 0.9091\n",
      "Epoch 137/200\n",
      "1/1 - 0s - loss: 0.4312 - accuracy: 0.9091\n",
      "Epoch 138/200\n",
      "1/1 - 0s - loss: 0.4239 - accuracy: 0.9091\n",
      "Epoch 139/200\n",
      "1/1 - 0s - loss: 0.4168 - accuracy: 0.9091\n",
      "Epoch 140/200\n",
      "1/1 - 0s - loss: 0.4099 - accuracy: 0.9091\n",
      "Epoch 141/200\n",
      "1/1 - 0s - loss: 0.4031 - accuracy: 0.9091\n",
      "Epoch 142/200\n",
      "1/1 - 0s - loss: 0.3964 - accuracy: 0.9091\n",
      "Epoch 143/200\n",
      "1/1 - 0s - loss: 0.3899 - accuracy: 0.9091\n",
      "Epoch 144/200\n",
      "1/1 - 0s - loss: 0.3835 - accuracy: 0.9091\n",
      "Epoch 145/200\n",
      "1/1 - 0s - loss: 0.3773 - accuracy: 0.9091\n",
      "Epoch 146/200\n",
      "1/1 - 0s - loss: 0.3712 - accuracy: 0.9091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 147/200\n",
      "1/1 - 0s - loss: 0.3652 - accuracy: 0.9091\n",
      "Epoch 148/200\n",
      "1/1 - 0s - loss: 0.3594 - accuracy: 0.9091\n",
      "Epoch 149/200\n",
      "1/1 - 0s - loss: 0.3536 - accuracy: 0.9091\n",
      "Epoch 150/200\n",
      "1/1 - 0s - loss: 0.3480 - accuracy: 0.9091\n",
      "Epoch 151/200\n",
      "1/1 - 0s - loss: 0.3425 - accuracy: 0.9091\n",
      "Epoch 152/200\n",
      "1/1 - 0s - loss: 0.3372 - accuracy: 0.9091\n",
      "Epoch 153/200\n",
      "1/1 - 0s - loss: 0.3319 - accuracy: 0.9091\n",
      "Epoch 154/200\n",
      "1/1 - 0s - loss: 0.3268 - accuracy: 0.9091\n",
      "Epoch 155/200\n",
      "1/1 - 0s - loss: 0.3217 - accuracy: 0.9091\n",
      "Epoch 156/200\n",
      "1/1 - 0s - loss: 0.3168 - accuracy: 0.9091\n",
      "Epoch 157/200\n",
      "1/1 - 0s - loss: 0.3120 - accuracy: 0.9091\n",
      "Epoch 158/200\n",
      "1/1 - 0s - loss: 0.3072 - accuracy: 0.9091\n",
      "Epoch 159/200\n",
      "1/1 - 0s - loss: 0.3026 - accuracy: 0.9091\n",
      "Epoch 160/200\n",
      "1/1 - 0s - loss: 0.2981 - accuracy: 0.9091\n",
      "Epoch 161/200\n",
      "1/1 - 0s - loss: 0.2936 - accuracy: 0.9091\n",
      "Epoch 162/200\n",
      "1/1 - 0s - loss: 0.2893 - accuracy: 0.9091\n",
      "Epoch 163/200\n",
      "1/1 - 0s - loss: 0.2850 - accuracy: 0.9091\n",
      "Epoch 164/200\n",
      "1/1 - 0s - loss: 0.2809 - accuracy: 0.9091\n",
      "Epoch 165/200\n",
      "1/1 - 0s - loss: 0.2768 - accuracy: 0.9091\n",
      "Epoch 166/200\n",
      "1/1 - 0s - loss: 0.2728 - accuracy: 0.9091\n",
      "Epoch 167/200\n",
      "1/1 - 0s - loss: 0.2688 - accuracy: 0.9091\n",
      "Epoch 168/200\n",
      "1/1 - 0s - loss: 0.2650 - accuracy: 0.9091\n",
      "Epoch 169/200\n",
      "1/1 - 0s - loss: 0.2612 - accuracy: 0.9091\n",
      "Epoch 170/200\n",
      "1/1 - 0s - loss: 0.2575 - accuracy: 0.9091\n",
      "Epoch 171/200\n",
      "1/1 - 0s - loss: 0.2539 - accuracy: 0.9091\n",
      "Epoch 172/200\n",
      "1/1 - 0s - loss: 0.2504 - accuracy: 0.9091\n",
      "Epoch 173/200\n",
      "1/1 - 0s - loss: 0.2469 - accuracy: 0.9091\n",
      "Epoch 174/200\n",
      "1/1 - 0s - loss: 0.2435 - accuracy: 0.9091\n",
      "Epoch 175/200\n",
      "1/1 - 0s - loss: 0.2401 - accuracy: 0.9091\n",
      "Epoch 176/200\n",
      "1/1 - 0s - loss: 0.2368 - accuracy: 0.9091\n",
      "Epoch 177/200\n",
      "1/1 - 0s - loss: 0.2336 - accuracy: 0.9091\n",
      "Epoch 178/200\n",
      "1/1 - 0s - loss: 0.2305 - accuracy: 0.9091\n",
      "Epoch 179/200\n",
      "1/1 - 0s - loss: 0.2274 - accuracy: 0.9091\n",
      "Epoch 180/200\n",
      "1/1 - 0s - loss: 0.2243 - accuracy: 0.9091\n",
      "Epoch 181/200\n",
      "1/1 - 0s - loss: 0.2213 - accuracy: 0.9091\n",
      "Epoch 182/200\n",
      "1/1 - 0s - loss: 0.2184 - accuracy: 0.9091\n",
      "Epoch 183/200\n",
      "1/1 - 0s - loss: 0.2155 - accuracy: 0.9091\n",
      "Epoch 184/200\n",
      "1/1 - 0s - loss: 0.2127 - accuracy: 1.0000\n",
      "Epoch 185/200\n",
      "1/1 - 0s - loss: 0.2099 - accuracy: 1.0000\n",
      "Epoch 186/200\n",
      "1/1 - 0s - loss: 0.2072 - accuracy: 1.0000\n",
      "Epoch 187/200\n",
      "1/1 - 0s - loss: 0.2045 - accuracy: 1.0000\n",
      "Epoch 188/200\n",
      "1/1 - 0s - loss: 0.2019 - accuracy: 1.0000\n",
      "Epoch 189/200\n",
      "1/1 - 0s - loss: 0.1993 - accuracy: 1.0000\n",
      "Epoch 190/200\n",
      "1/1 - 0s - loss: 0.1968 - accuracy: 1.0000\n",
      "Epoch 191/200\n",
      "1/1 - 0s - loss: 0.1942 - accuracy: 1.0000\n",
      "Epoch 192/200\n",
      "1/1 - 0s - loss: 0.1918 - accuracy: 1.0000\n",
      "Epoch 193/200\n",
      "1/1 - 0s - loss: 0.1894 - accuracy: 1.0000\n",
      "Epoch 194/200\n",
      "1/1 - 0s - loss: 0.1870 - accuracy: 1.0000\n",
      "Epoch 195/200\n",
      "1/1 - 0s - loss: 0.1846 - accuracy: 1.0000\n",
      "Epoch 196/200\n",
      "1/1 - 0s - loss: 0.1823 - accuracy: 1.0000\n",
      "Epoch 197/200\n",
      "1/1 - 0s - loss: 0.1800 - accuracy: 1.0000\n",
      "Epoch 198/200\n",
      "1/1 - 0s - loss: 0.1778 - accuracy: 1.0000\n",
      "Epoch 199/200\n",
      "1/1 - 0s - loss: 0.1756 - accuracy: 1.0000\n",
      "Epoch 200/200\n",
      "1/1 - 0s - loss: 0.1734 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x16ee559a988>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=200, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0b475ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_generation(model, t, current_word, n): # 모델, 토크나이저, 현재 단어, 반복핛 횟수\n",
    "    init_word = current_word # 처음 들어온 단어도 마지막에 같이 출력하기위해 저장\n",
    "    sentence = ''\n",
    "    for _ in range(n): # n번 반복\n",
    "        encoded = t.texts_to_sequences([current_word])[0] # 현재 단어에 대핚 정수 인코딩\n",
    "        encoded = pad_sequences([encoded], maxlen=5, padding='pre') # 데이터에 대핚 패딩\n",
    "        result = model.predict_classes(encoded, verbose=0)\n",
    "        # 입력핚 X(현재 단어)에 대해서 Y를 예측하고 Y(예측핚 단어)를 result에 저장.\n",
    "        #result = np.argmax(temp, axis=1)\n",
    "        for word, index in t.word_index.items(): \n",
    "            if index == result: # 맊약 예측핚 단어와 인덱스와 동일핚 단어가 있다면\n",
    "                break # 해당 단어가 예측 단어이므로 break\n",
    "        current_word = current_word + ' ' + word # 현재 단어 + ' ' + 예측 단어를 현재 단어로 변경\n",
    "        sentence = sentence + ' ' + word # 예측 단어를 문장에 저장\n",
    "        # for문이므로 이 행동을 다시 반복\n",
    "    sentence = init_word + sentence\n",
    "    return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8fb16831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "경마장에 있는 말이 뛰고 있다\n",
      "그의 말이 법이다\n",
      "가는 말이 고와야 오는 말이 곱다\n"
     ]
    }
   ],
   "source": [
    "print(sentence_generation(model, t, '경마장에', 4))\n",
    "print(sentence_generation(model, t, '그의', 2)) # 2번 예측\n",
    "print(sentence_generation(model, t, '가는', 5)) # 5번 예측\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c890c6",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e79d2f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "열의 개수:  15\n",
      "Index(['articleID', 'articleWordCount', 'byline', 'documentType', 'headline',\n",
      "       'keywords', 'multimedia', 'newDesk', 'printPage', 'pubDate',\n",
      "       'sectionName', 'snippet', 'source', 'typeOfMaterial', 'webURL'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from string import punctuation\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "df=pd.read_csv('ArticlesApril2018.csv') # 데이터 로드\n",
    "df.head()\n",
    "print('열의 개수: ',len(df.columns))\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "70e772ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['headline'].isnull().values.any()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0dc8754a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Former N.F.L. Cheerleaders’ Settlement Offer: $1 and a Meeting With Goodell',\n",
       " 'E.P.A. to Unveil a New Rule. Its Effect: Less Science in Policymaking.',\n",
       " 'The New Noma, Explained',\n",
       " 'Unknown',\n",
       " 'Unknown']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headline = [] # 리스트 선얶\n",
    "headline.extend(list(df.headline.values)) # 헤드라인의 값들을 리스트로 저장\n",
    "headline[:5] # 상위 5개 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8f824c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 샘플의 개수 : 1214\n",
      "노이즈값 제거 후 샘플의 개수 : 1214\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Former N.F.L. Cheerleaders’ Settlement Offer: $1 and a Meeting With Goodell',\n",
       " 'E.P.A. to Unveil a New Rule. Its Effect: Less Science in Policymaking.',\n",
       " 'The New Noma, Explained',\n",
       " 'How a Bag of Texas Dirt  Became a Times Tradition',\n",
       " 'Is School a Place for Self-Expression?']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'총 샘플의 개수 : {len(headline)}') # 현재 샘플의 개수\n",
    "headline = [n for n in headline if n != \"Unknown\"] # Unknown(잡음) 값을 가짂 샘플 제거\n",
    "print(f'노이즈값 제거 후 샘플의 개수 : {len(headline)}') # 제거 후 샘플의 개수\n",
    "headline[:5] # 5개의 샘플 출력 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ff8b35e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['former nfl cheerleaders settlement offer 1 and a meeting with goodell',\n",
       " 'epa to unveil a new rule its effect less science in policymaking',\n",
       " 'the new noma explained',\n",
       " 'how a bag of texas dirt  became a times tradition',\n",
       " 'is school a place for selfexpression']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def repreprocessing(s):\n",
    "    s=s.encode(\"utf8\").decode(\"ascii\",'ignore')\n",
    "    return ''.join(c for c in s if c not in punctuation).lower() # 구두점 제거와 동시에 소문자화\n",
    "\n",
    "text = [repreprocessing(x) for x in headline]\n",
    "text[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b244f444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기 : 3494\n"
     ]
    }
   ],
   "source": [
    "t = Tokenizer()\n",
    "t.fit_on_texts(text)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "print('단어 집합의 크기 : %d' % vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "93bf8535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[99, 269],\n",
       " [99, 269, 371],\n",
       " [99, 269, 371, 1115],\n",
       " [99, 269, 371, 1115, 582],\n",
       " [99, 269, 371, 1115, 582, 52],\n",
       " [99, 269, 371, 1115, 582, 52, 7],\n",
       " [99, 269, 371, 1115, 582, 52, 7, 2],\n",
       " [99, 269, 371, 1115, 582, 52, 7, 2, 372],\n",
       " [99, 269, 371, 1115, 582, 52, 7, 2, 372, 10],\n",
       " [99, 269, 371, 1115, 582, 52, 7, 2, 372, 10, 1116],\n",
       " [100, 3]]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = list()\n",
    "for line in text: \n",
    "    encoded = t.texts_to_sequences([line])[0] # 각 샘플에 대핚 정수 인코딩\n",
    "    for i in range(1, len(encoded)):\n",
    "        sequence = encoded[:i+1]\n",
    "        sequences.append(sequence)\n",
    "sequences[:11] # 11개의 샘플 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "40eacc64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "빈도수 상위 582번 단어 : offer\n",
      "샘플의 최대 길이 : 24\n"
     ]
    }
   ],
   "source": [
    "index_to_word={}\n",
    "for key, value in t.word_index.items(): # 인덱스를 단어로 바꾸기 위해 index_to_word를 생성\n",
    "    index_to_word[value] = key\n",
    "print(f'빈도수 상위 582번 단어 : {index_to_word[582]}') #.format(index_to_word[582]))\n",
    "\n",
    "max_len=max(len(l) for l in sequences) # 가장 긴 샘플의 길이 확인\n",
    "print(f'샘플의 최대 길이 : {max_len}') #.format(max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4eb31fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0   99  269]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0   99  269  371]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0   99  269  371 1115]]\n"
     ]
    }
   ],
   "source": [
    "# 가장 긴 샘플의 길이인 24로 모든 샘플의 길이를 패딩\n",
    "sequences = pad_sequences(sequences, maxlen=max_len, padding='pre') \n",
    "print(sequences[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e1651996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0  99]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0  99 269]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0  99 269 371]]\n",
      "[ 269  371 1115]\n"
     ]
    }
   ],
   "source": [
    "sequences = np.array(sequences)\n",
    "X = sequences[:,:-1] # 학습 데이터\n",
    "y = sequences[:,-1] # 정답 데이터\n",
    "print(X[:3])\n",
    "print(y[:3]) # 레이블 3개 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "08848636",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = to_categorical(y, num_classes=vocab_size) # 레이블 데이터 y에 대해서 원-핪 인코딩 수행\n",
    "y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4f7ae1e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "16/16 - 2s - loss: 8.1292 - accuracy: 0.0243\n",
      "Epoch 2/200\n",
      "16/16 - 1s - loss: 7.4772 - accuracy: 0.0243\n",
      "Epoch 3/200\n",
      "16/16 - 1s - loss: 7.1309 - accuracy: 0.0286\n",
      "Epoch 4/200\n",
      "16/16 - 1s - loss: 7.0785 - accuracy: 0.0317\n",
      "Epoch 5/200\n",
      "16/16 - 1s - loss: 7.0703 - accuracy: 0.0317\n",
      "Epoch 6/200\n",
      "16/16 - 1s - loss: 7.0631 - accuracy: 0.0317\n",
      "Epoch 7/200\n",
      "16/16 - 1s - loss: 7.0590 - accuracy: 0.0317\n",
      "Epoch 8/200\n",
      "16/16 - 1s - loss: 7.0538 - accuracy: 0.0317\n",
      "Epoch 9/200\n",
      "16/16 - 1s - loss: 7.0454 - accuracy: 0.0306\n",
      "Epoch 10/200\n",
      "16/16 - 1s - loss: 7.0348 - accuracy: 0.0317\n",
      "Epoch 11/200\n",
      "16/16 - 1s - loss: 7.0198 - accuracy: 0.0317\n",
      "Epoch 12/200\n",
      "16/16 - 1s - loss: 7.0032 - accuracy: 0.0326\n",
      "Epoch 13/200\n",
      "16/16 - 1s - loss: 6.9801 - accuracy: 0.0317\n",
      "Epoch 14/200\n",
      "16/16 - 1s - loss: 6.9563 - accuracy: 0.0317\n",
      "Epoch 15/200\n",
      "16/16 - 1s - loss: 6.9317 - accuracy: 0.0317\n",
      "Epoch 16/200\n",
      "16/16 - 1s - loss: 6.9095 - accuracy: 0.0319\n",
      "Epoch 17/200\n",
      "16/16 - 1s - loss: 6.8883 - accuracy: 0.0365\n",
      "Epoch 18/200\n",
      "16/16 - 1s - loss: 6.8685 - accuracy: 0.0382\n",
      "Epoch 19/200\n",
      "16/16 - 1s - loss: 6.8498 - accuracy: 0.0419\n",
      "Epoch 20/200\n",
      "16/16 - 1s - loss: 6.8332 - accuracy: 0.0375\n",
      "Epoch 21/200\n",
      "16/16 - 1s - loss: 6.8160 - accuracy: 0.0423\n",
      "Epoch 22/200\n",
      "16/16 - 1s - loss: 6.8005 - accuracy: 0.0423\n",
      "Epoch 23/200\n",
      "16/16 - 1s - loss: 6.7840 - accuracy: 0.0450\n",
      "Epoch 24/200\n",
      "16/16 - 1s - loss: 6.7690 - accuracy: 0.0423\n",
      "Epoch 25/200\n",
      "16/16 - 1s - loss: 6.7526 - accuracy: 0.0463\n",
      "Epoch 26/200\n",
      "16/16 - 1s - loss: 6.7364 - accuracy: 0.0454\n",
      "Epoch 27/200\n",
      "16/16 - 1s - loss: 6.7203 - accuracy: 0.0468\n",
      "Epoch 28/200\n",
      "16/16 - 1s - loss: 6.7039 - accuracy: 0.0459\n",
      "Epoch 29/200\n",
      "16/16 - 1s - loss: 6.6877 - accuracy: 0.0474\n",
      "Epoch 30/200\n",
      "16/16 - 1s - loss: 6.6706 - accuracy: 0.0473\n",
      "Epoch 31/200\n",
      "16/16 - 1s - loss: 6.6541 - accuracy: 0.0484\n",
      "Epoch 32/200\n",
      "16/16 - 1s - loss: 6.6392 - accuracy: 0.0472\n",
      "Epoch 33/200\n",
      "16/16 - 1s - loss: 6.6230 - accuracy: 0.0490\n",
      "Epoch 34/200\n",
      "16/16 - 1s - loss: 6.6073 - accuracy: 0.0478\n",
      "Epoch 35/200\n",
      "16/16 - 1s - loss: 6.5904 - accuracy: 0.0455\n",
      "Epoch 36/200\n",
      "16/16 - 1s - loss: 6.5753 - accuracy: 0.0514\n",
      "Epoch 37/200\n",
      "16/16 - 1s - loss: 6.5590 - accuracy: 0.0493\n",
      "Epoch 38/200\n",
      "16/16 - 1s - loss: 6.5427 - accuracy: 0.0490\n",
      "Epoch 39/200\n",
      "16/16 - 1s - loss: 6.5258 - accuracy: 0.0515\n",
      "Epoch 40/200\n",
      "16/16 - 1s - loss: 6.5092 - accuracy: 0.0514\n",
      "Epoch 41/200\n",
      "16/16 - 1s - loss: 6.4926 - accuracy: 0.0533\n",
      "Epoch 42/200\n",
      "16/16 - 1s - loss: 6.4759 - accuracy: 0.0547\n",
      "Epoch 43/200\n",
      "16/16 - 1s - loss: 6.4588 - accuracy: 0.0538\n",
      "Epoch 44/200\n",
      "16/16 - 1s - loss: 6.4419 - accuracy: 0.0554\n",
      "Epoch 45/200\n",
      "16/16 - 1s - loss: 6.4246 - accuracy: 0.0540\n",
      "Epoch 46/200\n",
      "16/16 - 1s - loss: 6.4070 - accuracy: 0.0555\n",
      "Epoch 47/200\n",
      "16/16 - 1s - loss: 6.3907 - accuracy: 0.0579\n",
      "Epoch 48/200\n",
      "16/16 - 1s - loss: 6.3721 - accuracy: 0.0556\n",
      "Epoch 49/200\n",
      "16/16 - 1s - loss: 6.3556 - accuracy: 0.0579\n",
      "Epoch 50/200\n",
      "16/16 - 1s - loss: 6.3382 - accuracy: 0.0570\n",
      "Epoch 51/200\n",
      "16/16 - 1s - loss: 6.3200 - accuracy: 0.0596\n",
      "Epoch 52/200\n",
      "16/16 - 1s - loss: 6.3033 - accuracy: 0.0581\n",
      "Epoch 53/200\n",
      "16/16 - 1s - loss: 6.2851 - accuracy: 0.0607\n",
      "Epoch 54/200\n",
      "16/16 - 1s - loss: 6.2673 - accuracy: 0.0613\n",
      "Epoch 55/200\n",
      "16/16 - 1s - loss: 6.2484 - accuracy: 0.0590\n",
      "Epoch 56/200\n",
      "16/16 - 1s - loss: 6.2304 - accuracy: 0.0616\n",
      "Epoch 57/200\n",
      "16/16 - 1s - loss: 6.2118 - accuracy: 0.0628\n",
      "Epoch 58/200\n",
      "16/16 - 1s - loss: 6.1945 - accuracy: 0.0636\n",
      "Epoch 59/200\n",
      "16/16 - 1s - loss: 6.1760 - accuracy: 0.0615\n",
      "Epoch 60/200\n",
      "16/16 - 1s - loss: 6.1579 - accuracy: 0.0607\n",
      "Epoch 61/200\n",
      "16/16 - 1s - loss: 6.1392 - accuracy: 0.0631\n",
      "Epoch 62/200\n",
      "16/16 - 1s - loss: 6.1205 - accuracy: 0.0628\n",
      "Epoch 63/200\n",
      "16/16 - 1s - loss: 6.1007 - accuracy: 0.0670\n",
      "Epoch 64/200\n",
      "16/16 - 1s - loss: 6.0809 - accuracy: 0.0633\n",
      "Epoch 65/200\n",
      "16/16 - 1s - loss: 6.0624 - accuracy: 0.0654\n",
      "Epoch 66/200\n",
      "16/16 - 1s - loss: 6.0441 - accuracy: 0.0669\n",
      "Epoch 67/200\n",
      "16/16 - 1s - loss: 6.0247 - accuracy: 0.0666\n",
      "Epoch 68/200\n",
      "16/16 - 1s - loss: 6.0055 - accuracy: 0.0670\n",
      "Epoch 69/200\n",
      "16/16 - 1s - loss: 5.9876 - accuracy: 0.0645\n",
      "Epoch 70/200\n",
      "16/16 - 1s - loss: 5.9671 - accuracy: 0.0691\n",
      "Epoch 71/200\n",
      "16/16 - 1s - loss: 5.9479 - accuracy: 0.0682\n",
      "Epoch 72/200\n",
      "16/16 - 1s - loss: 5.9287 - accuracy: 0.0689\n",
      "Epoch 73/200\n",
      "16/16 - 1s - loss: 5.9092 - accuracy: 0.0697\n",
      "Epoch 74/200\n",
      "16/16 - 1s - loss: 5.8901 - accuracy: 0.0688\n",
      "Epoch 75/200\n",
      "16/16 - 1s - loss: 5.8700 - accuracy: 0.0704\n",
      "Epoch 76/200\n",
      "16/16 - 1s - loss: 5.8512 - accuracy: 0.0723\n",
      "Epoch 77/200\n",
      "16/16 - 1s - loss: 5.8316 - accuracy: 0.0715\n",
      "Epoch 78/200\n",
      "16/16 - 1s - loss: 5.8127 - accuracy: 0.0719\n",
      "Epoch 79/200\n",
      "16/16 - 1s - loss: 5.7927 - accuracy: 0.0724\n",
      "Epoch 80/200\n",
      "16/16 - 1s - loss: 5.7749 - accuracy: 0.0709\n",
      "Epoch 81/200\n",
      "16/16 - 1s - loss: 5.7553 - accuracy: 0.0723\n",
      "Epoch 82/200\n",
      "16/16 - 1s - loss: 5.7350 - accuracy: 0.0729\n",
      "Epoch 83/200\n",
      "16/16 - 1s - loss: 5.7179 - accuracy: 0.0730\n",
      "Epoch 84/200\n",
      "16/16 - 1s - loss: 5.6971 - accuracy: 0.0727\n",
      "Epoch 85/200\n",
      "16/16 - 1s - loss: 5.6794 - accuracy: 0.0751\n",
      "Epoch 86/200\n",
      "16/16 - 1s - loss: 5.6596 - accuracy: 0.0741\n",
      "Epoch 87/200\n",
      "16/16 - 1s - loss: 5.6406 - accuracy: 0.0732\n",
      "Epoch 88/200\n",
      "16/16 - 1s - loss: 5.6214 - accuracy: 0.0734\n",
      "Epoch 89/200\n",
      "16/16 - 1s - loss: 5.6040 - accuracy: 0.0737\n",
      "Epoch 90/200\n",
      "16/16 - 1s - loss: 5.5840 - accuracy: 0.0747\n",
      "Epoch 91/200\n",
      "16/16 - 1s - loss: 5.5664 - accuracy: 0.0748\n",
      "Epoch 92/200\n",
      "16/16 - 1s - loss: 5.5477 - accuracy: 0.0751\n",
      "Epoch 93/200\n",
      "16/16 - 1s - loss: 5.5280 - accuracy: 0.0754\n",
      "Epoch 94/200\n",
      "16/16 - 1s - loss: 5.5110 - accuracy: 0.0766\n",
      "Epoch 95/200\n",
      "16/16 - 1s - loss: 5.4916 - accuracy: 0.0768\n",
      "Epoch 96/200\n",
      "16/16 - 1s - loss: 5.4730 - accuracy: 0.0773\n",
      "Epoch 97/200\n",
      "16/16 - 1s - loss: 5.4556 - accuracy: 0.0777\n",
      "Epoch 98/200\n",
      "16/16 - 1s - loss: 5.4364 - accuracy: 0.0766\n",
      "Epoch 99/200\n",
      "16/16 - 1s - loss: 5.4173 - accuracy: 0.0796\n",
      "Epoch 100/200\n",
      "16/16 - 1s - loss: 5.3999 - accuracy: 0.0777\n",
      "Epoch 101/200\n",
      "16/16 - 1s - loss: 5.3816 - accuracy: 0.0788\n",
      "Epoch 102/200\n",
      "16/16 - 1s - loss: 5.3635 - accuracy: 0.0784\n",
      "Epoch 103/200\n",
      "16/16 - 1s - loss: 5.3451 - accuracy: 0.0788\n",
      "Epoch 104/200\n",
      "16/16 - 1s - loss: 5.3275 - accuracy: 0.0797\n",
      "Epoch 105/200\n",
      "16/16 - 1s - loss: 5.3089 - accuracy: 0.0789\n",
      "Epoch 106/200\n",
      "16/16 - 1s - loss: 5.2912 - accuracy: 0.0797\n",
      "Epoch 107/200\n",
      "16/16 - 1s - loss: 5.2730 - accuracy: 0.0795\n",
      "Epoch 108/200\n",
      "16/16 - 1s - loss: 5.2550 - accuracy: 0.0830\n",
      "Epoch 109/200\n",
      "16/16 - 1s - loss: 5.2384 - accuracy: 0.0816\n",
      "Epoch 110/200\n",
      "16/16 - 1s - loss: 5.2210 - accuracy: 0.0820\n",
      "Epoch 111/200\n",
      "16/16 - 1s - loss: 5.2033 - accuracy: 0.0850\n",
      "Epoch 112/200\n",
      "16/16 - 1s - loss: 5.1850 - accuracy: 0.0862\n",
      "Epoch 113/200\n",
      "16/16 - 1s - loss: 5.1668 - accuracy: 0.0842\n",
      "Epoch 114/200\n",
      "16/16 - 1s - loss: 5.1498 - accuracy: 0.0879\n",
      "Epoch 115/200\n",
      "16/16 - 1s - loss: 5.1317 - accuracy: 0.0868\n",
      "Epoch 116/200\n",
      "16/16 - 1s - loss: 5.1148 - accuracy: 0.0879\n",
      "Epoch 117/200\n",
      "16/16 - 1s - loss: 5.0977 - accuracy: 0.0896\n",
      "Epoch 118/200\n",
      "16/16 - 1s - loss: 5.0796 - accuracy: 0.0897\n",
      "Epoch 119/200\n",
      "16/16 - 1s - loss: 5.0634 - accuracy: 0.0886\n",
      "Epoch 120/200\n",
      "16/16 - 1s - loss: 5.0455 - accuracy: 0.0900\n",
      "Epoch 121/200\n",
      "16/16 - 1s - loss: 5.0289 - accuracy: 0.0933\n",
      "Epoch 122/200\n",
      "16/16 - 1s - loss: 5.0111 - accuracy: 0.0923\n",
      "Epoch 123/200\n",
      "16/16 - 1s - loss: 4.9955 - accuracy: 0.0941\n",
      "Epoch 124/200\n",
      "16/16 - 1s - loss: 4.9772 - accuracy: 0.0939\n",
      "Epoch 125/200\n",
      "16/16 - 1s - loss: 4.9604 - accuracy: 0.0957\n",
      "Epoch 126/200\n",
      "16/16 - 1s - loss: 4.9438 - accuracy: 0.0973\n",
      "Epoch 127/200\n",
      "16/16 - 1s - loss: 4.9258 - accuracy: 0.0969\n",
      "Epoch 128/200\n",
      "16/16 - 1s - loss: 4.9095 - accuracy: 0.0997\n",
      "Epoch 129/200\n",
      "16/16 - 1s - loss: 4.8939 - accuracy: 0.0994\n",
      "Epoch 130/200\n",
      "16/16 - 1s - loss: 4.8771 - accuracy: 0.1011\n",
      "Epoch 131/200\n",
      "16/16 - 1s - loss: 4.8600 - accuracy: 0.1014\n",
      "Epoch 132/200\n",
      "16/16 - 1s - loss: 4.8436 - accuracy: 0.1041\n",
      "Epoch 133/200\n",
      "16/16 - 1s - loss: 4.8279 - accuracy: 0.1051\n",
      "Epoch 134/200\n",
      "16/16 - 1s - loss: 4.8112 - accuracy: 0.1065\n",
      "Epoch 135/200\n",
      "16/16 - 1s - loss: 4.7954 - accuracy: 0.1105\n",
      "Epoch 136/200\n",
      "16/16 - 1s - loss: 4.7782 - accuracy: 0.1115\n",
      "Epoch 137/200\n",
      "16/16 - 1s - loss: 4.7621 - accuracy: 0.1107\n",
      "Epoch 138/200\n",
      "16/16 - 1s - loss: 4.7468 - accuracy: 0.1114\n",
      "Epoch 139/200\n",
      "16/16 - 1s - loss: 4.7306 - accuracy: 0.1134\n",
      "Epoch 140/200\n",
      "16/16 - 1s - loss: 4.7148 - accuracy: 0.1179\n",
      "Epoch 141/200\n",
      "16/16 - 1s - loss: 4.6980 - accuracy: 0.1188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 142/200\n",
      "16/16 - 1s - loss: 4.6830 - accuracy: 0.1191\n",
      "Epoch 143/200\n",
      "16/16 - 1s - loss: 4.6667 - accuracy: 0.1189\n",
      "Epoch 144/200\n",
      "16/16 - 1s - loss: 4.6520 - accuracy: 0.1214\n",
      "Epoch 145/200\n",
      "16/16 - 1s - loss: 4.6366 - accuracy: 0.1219\n",
      "Epoch 146/200\n",
      "16/16 - 1s - loss: 4.6205 - accuracy: 0.1252\n",
      "Epoch 147/200\n",
      "16/16 - 1s - loss: 4.6046 - accuracy: 0.1285\n",
      "Epoch 148/200\n",
      "16/16 - 1s - loss: 4.5895 - accuracy: 0.1305\n",
      "Epoch 149/200\n",
      "16/16 - 1s - loss: 4.5742 - accuracy: 0.1328\n",
      "Epoch 150/200\n",
      "16/16 - 1s - loss: 4.5588 - accuracy: 0.1346\n",
      "Epoch 151/200\n",
      "16/16 - 1s - loss: 4.5429 - accuracy: 0.1352\n",
      "Epoch 152/200\n",
      "16/16 - 1s - loss: 4.5284 - accuracy: 0.1385\n",
      "Epoch 153/200\n",
      "16/16 - 1s - loss: 4.5132 - accuracy: 0.1385\n",
      "Epoch 154/200\n",
      "16/16 - 1s - loss: 4.4978 - accuracy: 0.1416\n",
      "Epoch 155/200\n",
      "16/16 - 1s - loss: 4.4821 - accuracy: 0.1433\n",
      "Epoch 156/200\n",
      "16/16 - 1s - loss: 4.4669 - accuracy: 0.1473\n",
      "Epoch 157/200\n",
      "16/16 - 1s - loss: 4.4512 - accuracy: 0.1456\n",
      "Epoch 158/200\n",
      "16/16 - 1s - loss: 4.4372 - accuracy: 0.1476\n",
      "Epoch 159/200\n",
      "16/16 - 1s - loss: 4.4217 - accuracy: 0.1498\n",
      "Epoch 160/200\n",
      "16/16 - 1s - loss: 4.4070 - accuracy: 0.1534\n",
      "Epoch 161/200\n",
      "16/16 - 1s - loss: 4.3912 - accuracy: 0.1565\n",
      "Epoch 162/200\n",
      "16/16 - 1s - loss: 4.3779 - accuracy: 0.1579\n",
      "Epoch 163/200\n",
      "16/16 - 1s - loss: 4.3640 - accuracy: 0.1598\n",
      "Epoch 164/200\n",
      "16/16 - 1s - loss: 4.3491 - accuracy: 0.1608\n",
      "Epoch 165/200\n",
      "16/16 - 1s - loss: 4.3345 - accuracy: 0.1644\n",
      "Epoch 166/200\n",
      "16/16 - 1s - loss: 4.3208 - accuracy: 0.1680\n",
      "Epoch 167/200\n",
      "16/16 - 1s - loss: 4.3056 - accuracy: 0.1713\n",
      "Epoch 168/200\n",
      "16/16 - 1s - loss: 4.2929 - accuracy: 0.1701\n",
      "Epoch 169/200\n",
      "16/16 - 1s - loss: 4.2779 - accuracy: 0.1696\n",
      "Epoch 170/200\n",
      "16/16 - 1s - loss: 4.2638 - accuracy: 0.1726\n",
      "Epoch 171/200\n",
      "16/16 - 1s - loss: 4.2472 - accuracy: 0.1795\n",
      "Epoch 172/200\n",
      "16/16 - 1s - loss: 4.2351 - accuracy: 0.1792\n",
      "Epoch 173/200\n",
      "16/16 - 1s - loss: 4.2194 - accuracy: 0.1852\n",
      "Epoch 174/200\n",
      "16/16 - 1s - loss: 4.2060 - accuracy: 0.1858\n",
      "Epoch 175/200\n",
      "16/16 - 1s - loss: 4.1923 - accuracy: 0.1860\n",
      "Epoch 176/200\n",
      "16/16 - 1s - loss: 4.1785 - accuracy: 0.1871\n",
      "Epoch 177/200\n",
      "16/16 - 1s - loss: 4.1638 - accuracy: 0.1906\n",
      "Epoch 178/200\n",
      "16/16 - 1s - loss: 4.1512 - accuracy: 0.1943\n",
      "Epoch 179/200\n",
      "16/16 - 1s - loss: 4.1372 - accuracy: 0.1953\n",
      "Epoch 180/200\n",
      "16/16 - 1s - loss: 4.1238 - accuracy: 0.1989\n",
      "Epoch 181/200\n",
      "16/16 - 1s - loss: 4.1097 - accuracy: 0.2017\n",
      "Epoch 182/200\n",
      "16/16 - 1s - loss: 4.0962 - accuracy: 0.2020\n",
      "Epoch 183/200\n",
      "16/16 - 1s - loss: 4.0839 - accuracy: 0.2034\n",
      "Epoch 184/200\n",
      "16/16 - 1s - loss: 4.0697 - accuracy: 0.2066\n",
      "Epoch 185/200\n",
      "16/16 - 1s - loss: 4.0563 - accuracy: 0.2104\n",
      "Epoch 186/200\n",
      "16/16 - 1s - loss: 4.0429 - accuracy: 0.2090\n",
      "Epoch 187/200\n",
      "16/16 - 1s - loss: 4.0306 - accuracy: 0.2106\n",
      "Epoch 188/200\n",
      "16/16 - 1s - loss: 4.0174 - accuracy: 0.2113\n",
      "Epoch 189/200\n",
      "16/16 - 1s - loss: 4.0031 - accuracy: 0.2153\n",
      "Epoch 190/200\n",
      "16/16 - 1s - loss: 3.9907 - accuracy: 0.2191\n",
      "Epoch 191/200\n",
      "16/16 - 1s - loss: 3.9775 - accuracy: 0.2216\n",
      "Epoch 192/200\n",
      "16/16 - 1s - loss: 3.9640 - accuracy: 0.2211\n",
      "Epoch 193/200\n",
      "16/16 - 1s - loss: 3.9509 - accuracy: 0.2266\n",
      "Epoch 194/200\n",
      "16/16 - 1s - loss: 3.9376 - accuracy: 0.2257\n",
      "Epoch 195/200\n",
      "16/16 - 1s - loss: 3.9255 - accuracy: 0.2268\n",
      "Epoch 196/200\n",
      "16/16 - 1s - loss: 3.9120 - accuracy: 0.2349\n",
      "Epoch 197/200\n",
      "16/16 - 1s - loss: 3.8988 - accuracy: 0.2370\n",
      "Epoch 198/200\n",
      "16/16 - 1s - loss: 3.8877 - accuracy: 0.2385\n",
      "Epoch 199/200\n",
      "16/16 - 1s - loss: 3.8742 - accuracy: 0.2408\n",
      "Epoch 200/200\n",
      "16/16 - 1s - loss: 3.8621 - accuracy: 0.2411\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x16e40e0a048>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "model = Sequential()\n",
    "# y데이터를 분리하였으므로 이제 X데이터의 길이는 기존 데이터의 길이 - 1\n",
    "model.add(Embedding(vocab_size, 10, input_length=max_len-1))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=200, batch_size=512, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2503d6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_generation(model, t, current_word, n): # 모델, 토크나이저, 현재 단어, 반복핛 횟수\n",
    "    init_word = current_word # 처음 들어온 단어도 마지막에 같이 출력하기위해 저장\n",
    "    sentence = ''\n",
    "    for _ in range(n): # n번 반복\n",
    "        encoded = t.texts_to_sequences([current_word])[0] # 현재 단어에 대핚 정수 인코딩\n",
    "        encoded = pad_sequences([encoded], maxlen=23, padding='pre') # 데이터에 대핚 패딩\n",
    "        result = model.predict_classes(encoded, verbose=0)\n",
    "        for word, index in t.word_index.items(): \n",
    "            if index == result: # 맊약 예측핚 단어와 인덱스와 동일핚 단어가 있다면\n",
    "                break # 해당 단어가 예측 단어이므로 break\n",
    "        current_word = current_word + ' ' + word # 현재 단어 + ' ' + 예측 단어를 현재 단어로 변경\n",
    "        sentence = sentence + ' ' + word # 예측 단어를 문장에 저장\n",
    "    sentence = init_word + sentence\n",
    "    return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "073ef4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i the family nights tries for the camera warriors wonkish near\n"
     ]
    }
   ],
   "source": [
    "# 임의의 단어 'i'에 대해서 10개의 단어를 추가 생성\n",
    "print(sentence_generation(model, t, 'i', 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3089031f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how the new mitt on the grid with the nra backward\n"
     ]
    }
   ],
   "source": [
    "# 임의의 단어 'how'에 대해서 10개의 단어를 추가 생성\n",
    "print(sentence_generation(model, t, 'how', 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8691d811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whats the ancient yorker chancellor a surplus of frowns antidepressants the\n"
     ]
    }
   ],
   "source": [
    "# 임의의 단어 'how'에 대해서 10개의 단어를 추가 생성\n",
    "print(sentence_generation(model, t, 'whats', 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1beefbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
